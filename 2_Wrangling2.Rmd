---
output:
  pdf_document: default
  html_document: default
---

```{r w_setup, warning=FALSE,hide=TRUE,echo=FALSE}

# Load the datasets which were saved on disk after using the course source code.
if(!exists("edx"))        edx <- readRDS("datasets/edx.rds")
if(!exists("validation")) validation <- readRDS("datasets/validation.rds")
```


## Description of the dataset

```{r w_create_extract}
# Starting point is to have a working set. Initial work is done on a sample of the full
# datasets. Dealing with millions of rows is too time consuming
USE_EXTRACT <- TRUE
PCT_EXTRACT <- 5./100. 

# Creates a working dataset using a full or partial extract, with timestamps converted to 
# days since 1-Jan-1915, being the oldest movie in the set ('The Birth of a Notion')
create_dataset <- function(dat){
  if (USE_EXTRACT){
    # If working with an extract.
    set.seed(1, sample.kind="Rounding")
    tmp_index <- createDataPartition(y = dat$rating, 
                                     times = 1, 
                                     p = PCT_EXTRACT,
                                     list = FALSE) 
    final <- dat[tmp_index,]
  } else {
    # If working with the full dataset
    final <- dat
  }  
    
  # Convert timestamp to number of days from start of 1-Jan-2015
  final <- final %>% 
    mutate(date_rating = ceiling_date( as_date(timestamp), unit = "day"))

  return(final)
}  

# Those are the datasets what will be used.
edx_data_starting        <- create_dataset(edx)
validation_data_starting <- create_dataset(validation)

```


The data provided is a list of ratings made by anonymised users of a number of movies. The training dataset is a table of `r nrow(edx)` rows and `r ncol(edx)` variables. Note that the dataset is extermely sparse: if each user had rated each movie, the dataset should contain `r nrow(edx) * ncol(edx)`, i.e. 85 times more. 

Each row represents a single rating made by a given user regarding a given movie. 

```{r echo=FALSE}
nMovies <- edx %>% select(movieId) %>% n_distinct() 
nUsers <-  edx %>% select(userId) %>% n_distinct()

# Check that a user never rated the same movie twice
# edx_data %>% group_by(movieId, userId) %>% n_distinct() == edx_data %>% group_by(movieId, userId) %>% nrow()
```

The complete data set includes  `r nMovies` unique movies, rated by `r nUsers` unique
users. No user rated the same movie twice ^[See source code.]. Importantly, the dataset is fully and properly populated: no missing or abnormal value was found.

The dataset variables are:

```{r variables_table,message=TRUE,cache=FALSE}

##
## This is the only way found to have something that looks good _both_ on html and pdf.
## http://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf
##
##
local({
  tbl <- data.frame(
    Name =        c("`userId`", 
                    "`movieId`", 
                    "`rating`", 
                    "`timestamp`", 
                    "`title`", 
                    "`genres`"),
    
    Format =      c("Numerical", 
                    "Numerical",
                    "Numerical", 
                    "Numerical", 
                    "Character string", 
                    "Character string"),
    
    Description = c("Unique numerical identifier for anonymity purposes", 
                    "Unique numerical identifier", "Possible ratings are 0, 0.5, 1, ..., 4.5 and 5.0. No movie is rated 0.", 
                    "Unix epoch of the date/time of the rating (i.e. number of seconds since 1-Jan-1970.", 
                    "String of characters of the movie title _AND_, in brackets, of the year the movie came out.",
                    "String of characters listing the genres to which the movie belongs. There are 20 possible categories. Each movie can belong to several categories (e.g. Action and Comedy). If there are several categories,  there are listed separated by a vertical bar."
                    )
  )

  kable(tbl, "latex", booktabs = T) %>% 
    kable_styling(full_width = F) %>% 
    column_spec(3, width = "8cm") %>% 
    row_spec(0, bold = T)
  
})

```



## Description of the variables.

The dataset needs to be preprocessed to add more practical information. Some steps are necessary to make available information usable: this is the case for splitting the genres and extracting the year a movie came out. Other changes are driven by the following considerations.

All users are resource-constrained. Watching a movie requires time and money, both of which are in limited supply. The act of taking the time to watch a movie, by itself, is an act of choice. The choice of which movie to watch results from a selection process that already biases a spectator towards movies he/she feels likely to enjoy. In other words, at least on an intuitive level, the pairs user/movie are not random: users did not select a movie randomly before rating it. 

From personal experience, we believe that:

+ A movie screened for the first time will sometimes be heavily marketed: the decision to watch this movie might be driven by hype rather than a reasoned choice.

+ In the medium term after first screening, movie availability could be relevant. Nowadays, internet gives access to a huge library of recent and not so recent movies. This was definitely not the case in the years at which ratings started to be collected (mid-nineties).

+ The decision to watch a movie that came out decades ago is a very deliberate process of choice. There is a _survival effect_ in the sense that time sieved out bad movies. We could expect old movies, e.g. _Citizen Kane_, to be rated higher on average than recent ones. 

+ In the short term, just a few weeks would make a difference on how a movie is perceived. But whether a movie is 50- or 55-year old would be of little impact. In other words, some sort of rescaling of time, logarithmic or other, need considering.

+ If a movie is very good, many people will watch it and rate it. In other words, we should see some correlation between ratings and numbers of ratings. Again, 


Whether this additional information is actually useful will be analysed later in this report.


#### Changes related to the movies:

+ Split the `genres` tags into separate logical variable, i.e. 1 variable per 
  individual genre. Each individual tags is a `0` or `1` numerical value, with `1` indicating that a movie belongs to that genre. The reason for using a numerical value is two-fold:

  - The key algorithm for recommender system is Singular Value Decomposition which requires all variable to be numerical (factors are not possibles). 
  - On a more intuitive level, movie are not all-or-nothing of a particular genre: a movie is not funny or not-funny; it could be a little bit funny or extremely funny. We could imagine a dataset where that movie would be a 20% or a 95% Comedy, possibly by extracting information from movies reviews. 
  
+ Recommender systems require variable scaling: for a given movie, all the ratings received by that movie are centered and scaled into a z-score. If a movie only received a single rating, the standard deviation is assumed to be 1 to avoid any missing value.

+ The date a movie came out is extracted from the title of the movie. The date is always a year, which we convert into Jauary, 1st of that year (to avoid any rating being dated before).
  
#### Changes related to the users:

+ As for the movies, for a given user,  ratings given by a particular user are centered and scaled using the mean and standard deviation of all the ratings given by that particular user. 

#### Changes related to the dates:

+ Timestamps at not very informative. All dates (including the date a movie came out) are converted to number of days. To avoid negative dates, the number of days begins from 1-January-1915, this being the earliest date appearing in the dataset (the year _The Birth of a Nation_ came out).

+ As we will see, ratings for older movies tend to be higher. Time lapsed until a movie is rated seems of interest (later analysis will shoew to which extent). The dataset is completed by there time lapses: looking at the date of a particular rating, how many days have passed since:

  - the movie came out;
  - the movie received its first rating;
  - the user gave its first rating.

+ All dates are also in [logarithmic / square root scale].


```{r w_create_userDB}
###################################################################################################
##
## Pre-processing of the data set.
## 
## Two additional datasets are first created:
##   - a database of movies containing informating specific to movies (movieDB)
##   - a database of users (userDB)


###################################################################################################
##
## Variable names:
## 
## The initial data downloaded from grouplens.org is edx and validation.
## 
## This is immediately copied to edx_data_starting and validation_data_starting  to avoid ever
## modifying the original data.
## 
## movieDB and userDB are created from edx_data_starting, validation_data_starting
## 
## The final training and test datasets (edx_data and validation_data) will included all the 
## variables from all datasets.
## 
## Regarding the test dataset, we could have a situation where a movie or a user appears it it 
## although they are not part of the training set. We then would not know what that new movie mean
## rating would be. To address this, we also need a _default movie_ and a _default user_ to fill
## those gaps. Those defaults are created from the training set.
## 



# Creates a database of movie based on the edx_data information.
# For each user:
# - userID
# - nrating: numbers of ratings made
# - mean of all ratings made by that user
# - standard deviation
# - median
# - date of the first rating
# 
# Then remove all variables unrelated to users 
userDB <- edx_data_starting %>% 
  group_by(userId) %>% 
  mutate(user_nRating = n(), 
         user_nRating_log = log10(user_nRating),
         user_mean_rating = mean(rating),
         s = sd(rating),                                        # temporary variable
         user_sd_rating   = if_else(is.na(s) | s == 0, 1, s),
         user_median_rating = median(rating), 
         user_first_rating = min(date_rating),
         user_z = (rating - user_mean_rating) / user_sd_rating) %>% 
  ungroup() %>%
  select(-movieId, -s, -timestamp, -title, -genres) %>% 
  select(-rating, -date_rating) %>% 
  distinct(userId, .keep_all = TRUE) %>% 
  arrange(userId)

# Create a default user if the validation data includes users that did not appear in the
# training ste
userDefault <- userDB %>% 
  select(-userId) %>% 
  summarise_all(mean)
```


```{r w_create_movieDB}

# Creates a database of movie based on the edx_data information similar to the users' one
movieDB <- edx_data_starting %>% 
  group_by(movieId) %>% 
  mutate(movie_nRating = n(), 
         movie_nRating_log = log10(movie_nRating),
         movie_mean_rating = mean(rating),
         s = sd(rating),
         movie_sd_rating = if_else(is.na(s) | s == 0, 1, s),
         movie_median_rating = median(rating), 
         movie_first_rating = min(date_rating), 
         movie_z = (rating - movie_mean_rating) / movie_sd_rating) %>% 
  ungroup() %>% 
  select(-userId, -rating, -timestamp, -s, -date_rating) %>% 
  distinct(movieId, .keep_all = TRUE) %>% 
  arrange(movieId)

# Add the date out a movie came out base on the title (year inside the brackets)
movieDB <- movieDB %>% 
  mutate(movie_year_out = str_match(title, "\\(\\d{4}\\)")) %>%
  mutate(movie_year_out = str_sub(movie_year_out, start = 2, end = 5)) %>%
  mutate(movie_date_out = as_datetime(paste0(movie_year_out, "-01-01"))) %>% 
  
  # Check that movie came out before the first reviews (few movies like that...)
  mutate(movie_date_out = if_else(movie_first_rating < movie_date_out, movie_first_rating, movie_date_out)) %>% 
  mutate(movie_year_out = year(movie_year_out))

# Snapshot1 made here
# save.image(file = "datasets/snapshot1.rda")
# load(file = "datasets/snapshot1.rda")


# List of all the genres (see READ.html in the dataset original zip file)
genres_list <- c("Action", "Adventure", "Animation", "Children", "Comedy", "Crime", 
                 "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", 
                 "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western")
  
# For each of the name in the list of genres
for(g in genres_list) {
  # The name of a tibble column cannot contain the "-" character
  predictor_name <- str_replace_all(g, "-", "")

  # Creates a new column named `predictor_name` with `0` or `1` depending if the genre 
  # `g` is detected. 
  # Note the use of "!!" and ":=" to use the string g which needs to be evaluated to the 
  # actual name of a genre. Refer to https://tidyeval.tidyverse.org/dplyr.html and
  #  https://dplyr.tidyverse.org/articles/programming.html for quoting/unquoting magic.
  movieDB <- movieDB %>%
    mutate(!! predictor_name := if_else(str_detect(genres, g), 1, 0))
}

# Create a default movie if the validation data includes movies that did not appear in the
# training ser. Simply the mean of all fields, absent no better information.
movieDefault <- movieDB %>% 
  select(-movieId, -title, -genres) %>% 
  summarise_all(mean)

# Snapshot2 made here
# save.image(file = "datasets/snapshot2.rda")
# load(file = "datasets/snapshot2.rda")
```


```{r w_create_edx}
# finalise the training set with that information
db <- movieDB %>% 
  select(-title, -genres)

# Training set is the downloaded dataset augmented with the movie and user specific information
# And add a z-score for ratings (across _all_ ratings).
rating_mean <- mean(edx_data_starting$rating)
rating_sd <- sd(edx_data_starting$rating)
  
edx_data <-  edx_data_starting %>%
  mutate(rating_z = (rating - rating_mean) / rating_sd) %>% 
  left_join(db, by = "movieId") %>% 
  left_join(userDB, by = "userId")

# Add time elapsed from date out, movie first rating and user first rating, including log/swuare root scale
edx_data <- edx_data %>% 
  mutate(time_since_out        = as.numeric(date_rating - movie_date_out), 
         time_since_out_log    = if_else(time_since_out == 0, -5, log10(time_since_out)), 
         time_since_out_sqrt   = sqrt(time_since_out), 
         
         time_movie_first      = as.numeric(date_rating - movie_first_rating), 
         time_movie_first_log  = if_else(time_movie_first == 0, -5, log10(time_movie_first)), 
         time_movie_first_sqrt = sqrt(time_movie_first),
         
         time_user_first       = as.numeric(date_rating - user_first_rating), 
         time_user_first_log   = if_else(time_user_first == 0, -5, log10(time_user_first)), 
         time_user_first_sqrt  = sqrt(time_user_first))


```


```{r w_create_validation}
# finalise the test set with that information

# Add movie information for movies in the test set which wree not in the training
# Adds a default movie (defined above)
db <- movieDB %>% 
  select(-title, -genres)

# Starting from downloaded data, adds movie specific information, but only for movies
# which exist in the test set and in the movie database (therefore inner_join instead of 
# left_join)
validation_data <-  validation_data_starting %>% 
  inner_join(db, by = "movieId")%>% 
  filter(!is.na(movie_nRating))

# For all movie in test set, not in movieDB (selected by anti_join), use movieDefaul to fill 
# the gap.
validation_data <- validation_data_starting %>% 
  anti_join(movieDB, by = "movieId") %>% 
  cbind(movieDefault) %>% 
  rbind(validation_data)


# Ditto for missing users
validation_data_tmp <-  validation_data %>% 
  inner_join(userDB, by = "userId")

validation_data <-  validation_data %>% 
  anti_join(userDB, by = "userId") %>% 
  cbind(userDefault) %>% 
  rbind(validation_data_tmp)

# Finally adds the rating z-score using the TRAINING mean and standard deviation.
validation_data <- validation_data %>% 
  mutate(rating_z = (rating - rating_mean) / rating_sd)


# Add time elapsed from date out, movie first rating and user first rating, including log/swuare root scale
validation_data <- validation_data %>% 
  mutate(time_since_out        = date_rating - movie_date_out, 
         time_since_out_log    = if_else(time_since_out == 0, -5, log10(time_since_out)), 
         time_since_out_sqrt   = sqrt(time_since_out), 
         
         time_movie_first      = date_rating - movie_first_rating, 
         time_movie_first_log  = if_else(time_movie_first == 0, -5, log10(time_movie_first)), 
         time_movie_first_sqrt = sqrt(time_movie_first),
         
         time_user_first       = date_rating - user_first_rating, 
         time_user_first_log   = if_else(time_user_first == 0, -5, log10(time_user_first)), 
         time_user_first_sqrt  = sqrt(time_user_first))


# Delete temporary variable
rm(db)
  

# Snapshot3 made here
# save.image(file = "datasets/snapshot3.rda")
# load(file = "datasets/snapshot3.rda")

```





### Users

There are `r nUsers` unique users. Many of them have submitted numerous ratings. Note that the Movielens dataset only contains users that have submitted more than 20 ratings ^[http://files.grouplens.org/datasets/movielens/ml-10m-README.html]. 





### Movies




### Summary of the steps

Starting from the dataset provided, we will do the following:

- Load the dataset in full or partially (for performance / debugging reasons);
- Convert the timestamp of a rating to a `date` object;
- Calculate the date of the first rating for a given movie;
- Calculate for each rating how long has passed since the first rating was made;
- Extract the year a movie came out from the title of the movie;
- Calculate for each rating how long has passed since the movie came out;
- Split the `genres` tags into separate logical variable, i.e. 1 variable per 
  individual genre;
- Calculate mean and standard deviation per _movie_, then the z-score of each 
  rating scaled per _movie_;
- Calculate mean and standard deviation per _user_, then the z-score of each 
  rating scaled per _user_;

Those steps are shown in detail carried out on the training dataset 
(`edx_data`). Then reproduced for the test dataset (`validation_data`).

### Data preparation

This section starts with the initial dataset and prepares it for subsequent 
analysis.

#### Load the datasets

We will leave the `edx` and `validation` datasets untouched. We will work on a 
full or partial (for performance reasons) extract.


#### Conversion of timestamp and determine date of first rating

The dataset contains 2 dates: the time stamp of when the rating was given, and 
the year the movie came out (in brackets in the title.) The time stamp is 
converted into an actual date.


We also add the date of the first rating, and the time between a given rating 
and that first rating:


#### Extract when a movie came out

We now extract the year a movie came out from its title, and set this date as 
January, 1st of that year:

A quick check to make sure that there are no ratings made before a movie came out. No 
dataset is perfect... but no issues here.

```{r}
movieDB %>% 
  filter(movie_first_rating < movie_date_out) %>% 
  select(movieId, title)
```

#### Split the genres

We can anticipate that users enjoy particular styles of movies, that is liking a few 
Westerns, probably means liking many Westerns.

Therefore, it is useful to split genres from the concatenated `genres` predictor.
A new predictor is created for each genre, which were identified visually) and filled if 
the `genre` string contains this genre. If the genre is in the genres string, fills with 
`1`, `0` otherwise. Numerical values are used.

#### Create movie and user z-scores

Finally, we create z-scores of the ratings per movie and per user (centered and scaled).


