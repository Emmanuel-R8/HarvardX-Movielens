---
output:
  pdf_document: default
  html_document: default
---

```{r w_setup, warning=FALSE,hide=TRUE,echo=FALSE}

# Load the datasets which were saved on disk after using the course source code.
if(!exists("edx"))        edx <- readRDS("datasets/edx.rds")
if(!exists("validation")) validation <- readRDS("datasets/validation.rds")
```


```{r w_create_extract}


# Starting point is to have a working set. Initial work is done on a sample of the full
# datasets. Dealing with millions of rows is too time consuming
USE_EXTRACT <- TRUE
PCT_EXTRACT <- 0.20

# Creates a working dataset using a full or partial extract, with timestamps converted to 
# days since 1-Jan-1915, being the oldest movie in the set ('The Birth of a Notion')
create_dataset <- function(dat){
  if (USE_EXTRACT){
    # If working with an extract.
    set.seed(42, sample.kind="Rounding")
    tmp_index <- createDataPartition(y = dat$rating, 
                                     times = 1, 
                                     p = PCT_EXTRACT,
                                     list = FALSE) 
    final <- dat[tmp_index,]
  } else {
    # If working with the full dataset
    final <- dat
  }  
    
  return(final)
}  

# Those are the datasets what will be used.
edx_extract        <- create_dataset(edx) %>% 
  mutate(date_rating = floor_date( as_datetime(timestamp), unit = "day")) 

validation_extract <- create_dataset(validation) %>% 
  mutate(date_rating = floor_date( as_datetime(timestamp), unit = "day")) 

```

Unless specified, this section only uses a portion (20%) of the dataset for performance reasons. 

## Description of the dataset

The data provided is a list of ratings made by anonymised users of a number of movies. The entire training dataset is a table of `r nrow(edx)` rows and `r ncol(edx)` variables. Note that the dataset is extermely sparse: if each user had rated each movie, the dataset should contain `r nrow(edx) * ncol(edx)` ratings, i.e. 85 times more. 

Each row represents a single rating made by a given user regarding a given movie. 

```{r echo=FALSE}
nMovies <- edx %>% select(movieId) %>% n_distinct() 
nUsers <-  edx %>% select(userId) %>% n_distinct()

nMovies_extract <- edx_extract %>% select(movieId) %>% n_distinct() 
nUsers_extract <-  edx_extract %>% select(userId) %>% n_distinct()

# Check that a user never rated the same movie twice
# edx_data %>% group_by(movieId, userId) %>% n_distinct() == edx_data %>% group_by(movieId, userId) %>% nrow()
```

The complete dataset includes  `r nMovies` unique movies, rated by `r nUsers` unique users. No user rated the same movie twice ^[See source code.]. Importantly, the dataset is fully and properly populated: no missing or abnormal value was found. However, a few movies were rated before the movie came out: the date of such ratings falls in the year before the one in brackets in the title. In such case, the date of first screening is brought to the date of the first rating.

The reduced data set includes  `r nMovies_extract` unique movies, rated by `r nUsers_extract` unique
users. That is, very few users or movies are missed by restricting the dataset.

The dataset variables are:

```{r variables_table,message=TRUE}

##
## This is the only way found to have something that looks good _both_ on html and pdf.
## http://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf
##
##
local({
  tbl <- data.frame(
    Name =        c("`userId`", 
                    "`movieId`", 
                    "`rating`", 
                    "`timestamp`", 
                    "`title`", 
                    "`genres`"),
    
    Format =      c("Numerical", 
                    "Numerical",
                    "Numerical", 
                    "Numerical", 
                    "Character string", 
                    "Character string"),
    
    Description = c("Unique numerical identifier for anonymity purposes", 
                    "Unique numerical identifier", "Possible ratings are 0, 0.5, 1, ..., 4.5 and 5.0. No movie is rated 0.", 
                    "Unix epoch of the date/time of the rating (i.e. number of seconds since 1-Jan-1970.", 
                    "String of characters of the movie title _AND_, in brackets, of the year the movie came out.",
                    "String of characters listing the genres to which the movie belongs. There are 20 possible categories. Each movie can belong to several categories (e.g. Action and Comedy). If there are several categories,  there are listed separated by a vertical bar."
                    )
  )

  kable(tbl, "latex", booktabs = T) %>% 
    kable_styling(full_width = F) %>% 
    column_spec(3, width = "8cm") %>% 
    row_spec(0, bold = T)
  
})

```



## Description of the variables.

### Intuitive description of the pre-processing requirements

The dataset needs to be preprocessed to add more practical information. Some steps are necessary to make available information usable: this is the case for splitting the genres and extracting the year a movie came out. Other changes are driven by the following considerations.

_*All users are resource-constrained*_. Watching a movie requires time and money, both of which are in limited supply. The act of taking the time to watch a movie, by itself, is an act of choice. The choice of which movie to watch results from a selection process that already biases a spectator towards movies he/she feels likely to enjoy. In other words, at least on an intuitive level, the pairs user/movie are not random: users did not select a movie randomly before rating it. 

It is common knowledge that:

+ A movie screened for the first time will sometimes be heavily marketed: the decision to watch this movie might be driven by hype rather than a reasoned choice; the choice to watch it is not a rational choice and will lead to possible disappointments.

+ In the medium term after first screening, movie availability could be relevant. Nowadays, internet gives access to a huge library of recent and not so recent movies. This was definitely not the case in the years at which ratings started to be collected (mid-nineties).

+ The decision to watch a movie that came out decades ago is a very deliberate process of choice. There is a _survival effect_ in the sense that time sieved out bad movies. We could expect old movies, e.g. _Citizen Kane_, to be rated higher on average than recent ones. 

+ In the short term, just a few weeks would make a difference on how a movie is perceived. But whether a movie is 50- or 55-year old would be of little impact. In other words, some sort of rescaling of time, logarithmic or other, need considering.

+ If a movie is very good, more people will watch it and rate it. In other words, we should see some correlation between ratings and numbers of ratings. Again, some sort of rescaling of time, logarithmic or other, need considering.

Whether this additional information is actually useful will be analysed later in this report.


#### Changes related to the movies:

+ Split the `genres` tags into separate logical variable, i.e. 1 variable per 
  individual genre. Each individual tags is a `-1` or `1` numerical value, with `1` indicating that a movie belongs to that genre. The reasons for using numerical values are:

  - On a more intuitive level, movie are not all-or-nothing of a particular genre: a movie is not funny or not-funny; it could be a little bit funny or extremely funny. We could imagine a dataset where that movie would be a 20% or a 95% Comedy, or -50% anti-funny movie, possibly by extracting information from movies reviews.  
  - We could also encode with 0,1 instead of -1,1. Modeling has shown to be more effective with the -1,1 encoding.
  - Key algorithms for recommender system involve dimension reduction which requires all variable to be numerical (no factors). 
  
+ Dimension reduction require variable scaling: for a given movie, all the ratings received by that movie are centered and scaled into a z-score. If a movie only received a single rating, the standard deviation is assumed to be 1 to avoid any missing value.

+ The date a movie came out is extracted from the title of the movie. The date is always a year, which we convert into January, 1st of that year (to avoid any rating being dated before).
  



#### Changes related to the users:

+ As for the movies, for a given user,  ratings given by a particular user are centered and scaled using the mean and standard deviation of all the ratings given by that particular user. 

#### Changes related to the dates:

+ Timestamps cannot be readily understood. All dates (including the date a movie came out) are converted to number of `proper `Lubridate` date objects. Difference between dates are expressed in days.

+ As we will see, ratings for older movies tend to be higher. Time lapsed until a movie is rated seems of interest (later analysis will show to which extent). The dataset is completed by there time lapses: looking at the date of a particular rating, how many days have passed since:

  - the movie came out;
  - the movie received its first rating;
  - the user gave its first rating.

+ All dates are also in [logarithmic / square root scale].


```{r w_create_userDB}
###################################################################################################
##
## Pre-processing of the data set.
## 
## Two additional datasets are first created:
##   - a database of movies containing informating specific to movies (movieDB)
##   - a database of users (userDB)
##


###################################################################################################
##
## Variable names:
## 
## The initial data downloaded from grouplens.org is edx and validation.
## 
## This is immediately copied to edx_data_starting and validation_data_starting  to avoid ever
## modifying the original data.
## 
## movieDB and userDB are created from edx_data_starting, validation_data_starting
## 
## The final training and test datasets (edx_data and validation_data) will included all the 
## variables from all datasets.
## 
## Regarding the test dataset, we could have a situation where a movie or a user appears it it 
## although they are not part of the training set. We then would not know what that new movie mean
## rating would be. To address this, we also need a _default movie_ and a _default user_ to fill
## those gaps. Those defaults are created from the training set.
## 



# Creates a database of movie based on the edx_data information.
# For each user:
# - userID
# - nrating: numbers of ratings made
# - mean of all ratings made by that user
# - standard deviation
# - median
# - date of the first rating
# 
# Then remove all variables unrelated to users 
userDB <- edx_extract %>% 
  group_by(userId) %>% 
  mutate(user_nRating = n(), 
         user_nRating_log = log10(user_nRating),
         user_mean_rating = mean(rating),
         s = sd(rating),                                        # temporary variable
         user_sd_rating   = if_else(is.na(s) | s == 0, 1, s),
         user_first_rating = floor_date(as_datetime(min(timestamp)), unit = "day"),
         user_z = (rating - user_mean_rating) / user_sd_rating) %>% 
  ungroup() %>%
  select(-movieId, -s, -timestamp, -title, -genres) %>% 
  select(-rating, -date_rating) %>% 
  distinct(userId, .keep_all = TRUE) %>% 
  arrange(userId)

# Create a default user if the validation data includes users that did not appear in the
# training ste
userDefault <- userDB %>% 
  select(-userId) %>% 
  summarise_all(mean)
```


```{r w_create_movieDB}

# Creates a database of movie based on the edx_data information similar to the users' one
movieDB <- edx_extract %>% 
  group_by(movieId) %>% 
  mutate(movie_nRating = n(), 
         movie_nRating_log = log10(movie_nRating),
         movie_mean_rating = mean(rating),
         s = sd(rating),
         movie_sd_rating = if_else(is.na(s) | s == 0, 1, s),
         movie_first_rating = floor_date(as_datetime(min(timestamp)), unit = "day"), 
         movie_z = (rating - movie_mean_rating) / movie_sd_rating) %>% 
  ungroup() %>% 
  select(-userId, -rating, -timestamp, -s, -date_rating) %>% 
  distinct(movieId, .keep_all = TRUE) %>% 
  arrange(movieId)

# Add the date out a movie came out base on the title (year inside the brackets)
movieDB <- movieDB %>% 
  mutate(movie_year_out = str_match(title, "\\(\\d{4}\\)")) %>%
  mutate(movie_year_out = str_sub(movie_year_out, start = 2, end = 5)) %>%
  mutate(movie_date_out = as_datetime(paste0(movie_year_out, "-01-01"))) %>% 
  
  # Check that movie came out before the first reviews (few movies like that...)
  mutate(movie_date_out = if_else(time_length(movie_date_out %--% movie_first_rating) < 0, 
                                  movie_first_rating, 
                                  movie_date_out)) %>% 
  mutate(movie_year_out = year(movie_date_out))

# Snapshot1 made here
# save.image(file = "datasets/snapshot1.rda")
# load(file = "datasets/snapshot1.rda")


# List of all the genres (see READ.html in the dataset original zip file)
genres_list <- c("Action", "Adventure", "Animation", "Children", "Comedy", "Crime", 
                 "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", 
                 "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western")

# Same without dashes
genres_variables  <- genres_list %>%  str_replace_all("-", "")

# UNUSED: -1, 1 is more informative when modeling
# genres_list0 <- str_c(genres_list, "0")
# genres_variables0 <- genres_list0 %>%  str_replace_all("-", "")

# For each of the name in the list of genres
for(g in genres_list) {
  # The name of a tibble column cannot contain the "-" character
  predictor_name  <- str_replace_all(g, "-", "")

  # Creates a new column named `predictor_name` with `0` or `1` depending if the genre 
  # `g` is detected. 
  # 
  # Note the use of "!!" and ":=" to use the string g which needs to be evaluated to the 
  # actual name of a genre. Refer to https://tidyeval.tidyverse.org/dplyr.html and
  #  https://dplyr.tidyverse.org/articles/programming.html for quoting/unquoting magic.
  movieDB <- movieDB %>%
    mutate(!! predictor_name := if_else(str_detect(genres, g), 1, -1))
}

# Create a default movie if the validation data includes movies that did not appear in the
# training ser. Simply the mean of all fields, absent no better information.
movieDefault <- movieDB %>% 
  select(-movieId, -title, -genres) %>% 
  summarise_all(mean)

# Snapshot2 made here
# save.image(file = "datasets/snapshot2.rda")
# load(file = "datasets/snapshot2.rda")
```


```{r w_create_edx}
# finalise the training set with that information
db <- movieDB %>% 
  select(-title, -genres)

# Training set is the downloaded dataset augmented with the movie and user specific information
# And add a z-score for ratings (across _all_ ratings).
rating_mean <- mean(edx_extract$rating)
rating_sd <- sd(edx_extract$rating)
  
edx_extract <-  edx_extract %>%
  mutate(rating_z = (rating - rating_mean) / rating_sd) %>% 
  left_join(db, by = "movieId") %>% 
  left_join(userDB, by = "userId")

# Add time elapsed from date out, movie first rating and user first rating, including log/swuare root scale
edx_extract <- edx_extract %>% 
  mutate(time_since_out        = as.numeric(movie_date_out %--% date_rating) / 86400, 
         time_since_out        = if_else(time_since_out <= 0.01, 1, time_since_out),
         time_since_out_log    = log10(time_since_out), 
         
         time_movie_first      = as.numeric(movie_first_rating %--% date_rating) / 86400, 
         time_movie_first      = if_else(time_movie_first <= 0.01, 1, time_movie_first),
         time_movie_first_log  = log10(time_movie_first),
         
         time_user_first       = as.numeric(user_first_rating %--% date_rating) / 86400, 
         time_user_first       = if_else(time_user_first <= 0.01, 1, time_user_first),
         time_user_first_log   = log10(time_user_first)) 

# Remove unused variables
edx_extract <- edx_extract %>% select(-timestamp, -genres)


```


```{r w_create_validation}
# finalise the test set with that information

# Add movie information for movies in the test set which wree not in the training
# Adds a default movie (defined above)
db <- movieDB %>% 
  select(-title, -genres)

# Starting from downloaded data, adds movie specific information, but only for movies
# which exist in the test set and in the movie database (therefore inner_join instead of 
# left_join)
validation_data_tmp <-  validation_extract %>% 
  inner_join(db, by = "movieId")%>% 
  filter(!is.na(movie_nRating))

# For all movie in test set, not in movieDB (selected by anti_join), use movieDefaul to fill 
# the gap.
validation_extract <- validation_extract %>% 
  anti_join(movieDB, by = "movieId") %>% 
  cbind(movieDefault) %>% 
  rbind(validation_data_tmp)


# Ditto for missing users
validation_data_tmp <-  validation_extract %>% 
  inner_join(userDB, by = "userId")

validation_extract <-  validation_extract %>% 
  anti_join(userDB, by = "userId") %>% 
  cbind(userDefault) %>% 
  rbind(validation_data_tmp)

# Finally adds the rating z-score using the TRAINING mean and standard deviation.
validation_extract <- validation_extract %>% 
  mutate(rating_z = (rating - rating_mean) / rating_sd)



# Add time elapsed from date out, movie first rating and user first rating, including log/square root scale
validation_extract <- validation_extract %>% 
  mutate(time_since_out        = as.numeric(movie_date_out %--% date_rating) / 86400, 
         time_since_out        = if_else(time_since_out <= 0.01, 1, time_since_out),
         time_since_out_log    = log10(time_since_out), 
         
         time_movie_first      = as.numeric(movie_first_rating %--% date_rating) / 86400, 
         time_movie_first      = if_else(time_movie_first <= 0.01, 1, time_movie_first),
         time_movie_first_log  = log10(time_movie_first),
         
         time_user_first       = as.numeric(user_first_rating %--% date_rating) / 86400, 
         time_user_first       = if_else(time_user_first <= 0.01, 1, time_user_first),
         time_user_first_log   = log10(time_user_first)) 


# Remove unused variables
validation_extract <- validation_extract %>% select(-timestamp, -genres)


# Delete temporary variable
rm(db, validation_data_tmp)
  

# Snapshot3 made here
# save.image(file = "datasets/snapshot3.rda")
# load(file = "datasets/snapshot3.rda")

```

```{r}
# Also creates equivalent dataset where all genres are stacked into a `genre` variable. Will be useful
# for some visualisation

edx_gathered <- edx_extract %>% 
  pivot_longer(cols = genres_variables, names_to = "genre") %>% 
  filter(value == 1) %>% 
  select(-value)

validation_gathered <- validation_extract %>% 
  pivot_longer(cols = genres_variables, names_to = "genre") %>% 
  filter(value == 1) %>% 
  select(-value)



```



### Summary of the steps

Once the pre-processing is carried out, the dataset variables are: 

```{r}
colnames(edx_extract)
```


```{r eval=FALSE}

## 
## Replicate all the data wrangling to the full dataset for later use in the modeling.
## This was done once and saved to disk to save time. It can be executed line by line otherwise.
## 
## WARNING: the full datasets add up to a few GB !! 
## 

# Full user database
userDB_full <- edx %>% 
  group_by(userId) %>% 
  mutate(user_nRating = n(), 
         user_nRating_log = log10(user_nRating),
         user_mean_rating = mean(rating),
         s = sd(rating),                                        # temporary variable
         user_sd_rating   = if_else(is.na(s) | s == 0, 1, s),
         user_median_rating = median(rating), 
         user_first_rating =ceiling_date(as_datetime(min(timestamp)), unit = "day"),
         user_z = (rating - user_mean_rating) / user_sd_rating) %>% 
  ungroup() %>%
  select(-movieId, -s, -timestamp, -title, -genres) %>% 
  select(-rating, -date_rating) %>% 
  distinct(userId, .keep_all = TRUE) %>% 
  arrange(userId)

# Create a default user if the validation data includes users that did not appear in the
# training ste
userDefault_full <- userDB_full %>% 
  select(-userId) %>% 
  summarise_all(mean)




# Creates a database of movie based on the edx_data information similar to the users' one
movieDB_full <- edx %>% 
  group_by(movieId) %>% 
  mutate(movie_nRating = n(), 
         movie_nRating_log = log10(movie_nRating),
         movie_mean_rating = mean(rating),
         s = sd(rating),
         movie_sd_rating = if_else(is.na(s) | s == 0, 1, s),
         movie_median_rating = median(rating), 
         movie_first_rating = ceiling_date(as_datetime(min(timestamp)), unit = "day"), 
         movie_z = (rating - movie_mean_rating) / movie_sd_rating) %>% 
  ungroup() %>% 
  select(-userId, -rating, -timestamp, -s, -date_rating) %>% 
  distinct(movieId, .keep_all = TRUE) %>% 
  arrange(movieId)

# Add the date out a movie came out base on the title (year inside the brackets)
movieDB_full <- movieDB_full %>% 
  mutate(movie_year_out = str_match(title, "\\(\\d{4}\\)")) %>%
  mutate(movie_year_out = str_sub(movie_year_out, start = 2, end = 5)) %>%
  mutate(movie_date_out = as_datetime(paste0(movie_year_out, "-01-01"))) %>% 
  
  # Check that movie came out before the first reviews (few movies like that...)
  mutate(movie_date_out = if_else(time_length(movie_date_out %--% movie_first_rating) < 0, 
                                  movie_first_rating, 
                                  movie_date_out)) %>% 
  mutate(movie_year_out = year(movie_date_out))

# Snapshot1 made here
# save.image(file = "datasets/snapshot1.rda")
# load(file = "datasets/snapshot1.rda")


# List of all the genres (see READ.html in the dataset original zip file)
genres_list <- c("Action", "Adventure", "Animation", "Children", "Comedy", "Crime", 
                 "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror", "Musical", 
                 "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western")

# Same without dashes
genres_variables <- genres_list %>%  str_replace_all("-", "")
  
# For each of the name in the list of genres
for(g in genres_list) {
  # The name of a tibble column cannot contain the "-" character
  predictor_name <- str_replace_all(g, "-", "")

  # Creates a new column named `predictor_name` with `0` or `1` depending if the genre 
  # `g` is detected. 
  # Note the use of "!!" and ":=" to use the string g which needs to be evaluated to the 
  # actual name of a genre. Refer to https://tidyeval.tidyverse.org/dplyr.html and
  #  https://dplyr.tidyverse.org/articles/programming.html for quoting/unquoting magic.
  movieDB_full <- movieDB_full %>%
    mutate(!! predictor_name := if_else(str_detect(genres, g), 1, -1))
}

# Create a default movie if the validation data includes movies that did not appear in the
# training ser. Simply the mean of all fields, absent no better information.
movieDefault_full <- movieDB_full %>% 
  select(-movieId, -title, -genres) %>% 
  summarise_all(mean)





# finalise the training set with that information
db <- movieDB_full %>% 
  select(-title, -genres)

# Training set is the downloaded dataset augmented with the movie and user specific information
# And add a z-score for ratings (across _all_ ratings).
rating_mean <- mean(edx$rating)
rating_sd <- sd(edx$rating)
  
edx_full <-  edx %>%
  mutate(rating_z = (rating - rating_mean) / rating_sd) %>% 
  left_join(db, by = "movieId") %>% 
  left_join(userDB, by = "userId")

# Add time elapsed from date out, movie first rating and user first rating, including log/swuare root scale
edx_full <- edx_full %>% 
  mutate(time_since_out        = as.numeric(movie_date_out %--% date_rating) / 86400, 
         time_since_out        = if_else(time_since_out <= 0.01, 1, time_since_out),
         time_since_out_log    = log10(time_since_out), 
         time_since_out_sqrt   = sqrt(time_since_out), 
         
         time_movie_first      = as.numeric(movie_first_rating %--% date_rating) / 86400, 
         time_movie_first      = if_else(time_movie_first <= 0.01, 1, time_movie_first),
         time_movie_first_log  = log10(time_movie_first), 
         time_movie_first_sqrt = sqrt(time_movie_first),
         
         time_user_first       = as.numeric(user_first_rating %--% date_rating) / 86400, 
         time_user_first       = if_else(time_user_first <= 0.01, 1, time_user_first),
         time_user_first_log   = log10(time_user_first), 
         time_user_first_sqrt  = sqrt(time_user_first)) 


# finalise the test set with that information

# Add movie information for movies in the test set which wree not in the training
# Adds a default movie (defined above)
db <- movieDB_full %>% 
  select(-title, -genres)

# Starting from downloaded data, adds movie specific information, but only for movies
# which exist in the test set and in the movie database (therefore inner_join instead of 
# left_join)
validation_data_tmp <-  validation %>% 
  inner_join(db, by = "movieId")%>% 
  filter(!is.na(movie_nRating))

# For all movie in test set, not in movieDB (selected by anti_join), use movieDefaul to fill 
# the gap.
validation_full <- validation %>% 
  anti_join(movieDB, by = "movieId") %>% 
  cbind(movieDefault_full) %>% 
  rbind(validation_data_tmp)


# Ditto for missing users
validation_data_tmp <-  validation_full %>% 
  inner_join(userDB, by = "userId")

validation_full <-  validation_full %>% 
  anti_join(userDB, by = "userId") %>% 
  cbind(userDefault) %>% 
  rbind(validation_data_tmp)

# Finally adds the rating z-score using the TRAINING mean and standard deviation.
validation_full <- validation_full %>% 
  mutate(rating_z = (rating - rating_mean) / rating_sd)



# Add time elapsed from date out, movie first rating and user first rating, including log/square root scale
validation_full <- validation_full %>% 
  mutate(time_since_out        = as.numeric(movie_date_out %--% date_rating) / 86400, 
         time_since_out        = if_else(time_since_out <= 0.01, 1, time_since_out),
         time_since_out_log    = log10(time_since_out), 
         time_since_out_sqrt   = sqrt(time_since_out), 
         
         time_movie_first      = as.numeric(movie_first_rating %--% date_rating) / 86400, 
         time_movie_first      = if_else(time_movie_first <= 0.01, 1, time_movie_first),
         time_movie_first_log  = log10(time_movie_first), 
         time_movie_first_sqrt = sqrt(time_movie_first),
         
         time_user_first       = as.numeric(user_first_rating %--% date_rating) / 86400, 
         time_user_first       = if_else(time_user_first <= 0.01, 1, time_user_first),
         time_user_first_log   = log10(time_user_first), 
         time_user_first_sqrt  = sqrt(time_user_first)) 


# Delete temporary variable
rm(db, validation_data_tmp)
  
```




