[
["introduction.html", "HarvardX - PH125.9x Data Science: Capstone - Movie Lens HarvardX - PH125.9x Data Science Capstone Chapter 1 Introduction", " HarvardX - PH125.9x Data Science: Capstone - Movie Lens HarvardX - PH125.9x Data Science Capstone Emmanuel Rialland - https://github.com/Emmanuel_R8 November 12, 2019 Chapter 1 Introduction This report is a story of failing human intuitions and data science success. In brief, it demonstrates that statistical learning brings insights otherwise unavailable, and eventually achieves and RMSE of *0. This project is the first of two final projects of the HarvardX - PH125.9x Data Science course. Its purpose is the development of a recommender system for movie ratings using the Movie Lens dataset.1 Recommender systems are a class of statistical learning systems that analyse individual past choices and/or preferences to propose relevant information to make future choices. Typical systems would be propose additional items to purchase knowing past shopping activity, searches (e.g. Amazon), choice of books (e.g. GoodRead) or movies (Netflix). Broadly, recommender systems fall into two categories: collaborative filtering (user-based) which attempts to pool similar users together and guide a user’s recommendation given the pool’s preference. content-based filtering which attempts to pool similar contents (e.g. shopping carts, movie ratings) together and guide a user’s recommendation within a simila pools of content. In practice, those two approaches are mixed together. A general overview is available on Wikipedia2 and in the course materials.3 Being given a training and a validation dataset, we will attempt to minimise the Root Mean Sqared Error (RMSE) of predicted ratings for pairs of user/movie below 0.8649. We note that Netflix organised a competition spanning over several years to improve a recommender system which shares many similarities with this project (Bennett, Lanning, and others 2007). Papers published by teams who participated in that competition have guided some of this report. (Bennett, Lanning, and others 2007) (Bell, Koren, and Volinsky 2007) (Bell, Koren, and Volinsky 2008) (Koren 2009) (Töscher, Jahrer, and Bell 2009) (Piotte and Chabbert 2009) (Gower 2014) This report is organised as follows. In Section 2, we describe the dataset and add a number of possibly relevant predictors. Section 3 provides a number of visualistions. Section 4 proposes three models that will show to be poor performers. Section 5 is dedicated to a low-rank matrix factorisation estimated with a stochastic gradient descent. References "],
["data-summary-and-processing.html", "Chapter 2 Data Summary and Processing 2.1 Description of the dataset 2.2 Description of the variables.", " Chapter 2 Data Summary and Processing Unless specified, this section only uses a portion (20%) of the dataset for performance reasons. 2.1 Description of the dataset The data provided is a list of ratings made by anonymised users of a number of movies. The entire training dataset is a table of 9000055 rows and 6 variables. Note that the dataset is extermely sparse: if each user had rated each movie, the dataset should contain 54000330 ratings, i.e. 85 times more. Each row represents a single rating made by a given user regarding a given movie. The complete dataset includes 10677 unique movies, rated by 69878 unique users. No user rated the same movie twice.4 Importantly, the dataset is fully and properly populated: no missing or abnormal value was found. However, a few movies were rated before the movie came out: the date of such ratings falls in the year before the one in brackets in the title. In such case, the date of first screening is brought to the date of the first rating. The reduced data set includes 10225 unique movies, rated by 69750 unique users. That is, very few users or movies are missed by restricting the dataset. The dataset variables are: 2.2 Description of the variables. 2.2.1 Intuitive description of the pre-processing requirements The dataset needs to be preprocessed to add more practical information. Some steps are necessary to make available information usable: this is the case for splitting the genres and extracting the year a movie came out. Other changes are driven by the following considerations. All users are resource-constrained. Watching a movie requires time and money, both of which are in limited supply. The act of taking the time to watch a movie, by itself, is an act of choice. The choice of which movie to watch results from a selection process that already biases a spectator towards movies he/she feels likely to enjoy. In other words, at least on an intuitive level, the pairs user/movie are not random: users did not select a movie randomly before rating it. It is common knowledge that: A movie screened for the first time will sometimes be heavily marketed: the decision to watch this movie might be driven by hype rather than a reasoned choice; the choice to watch it is not a rational choice and will lead to possible disappointments. In the medium term after first screening, movie availability could be relevant. Nowadays, internet gives access to a huge library of recent and not so recent movies. This was definitely not the case in the years at which ratings started to be collected (mid-nineties). The decision to watch a movie that came out decades ago is a very deliberate process of choice. There is a survival effect in the sense that time sieved out bad movies. We could expect old movies, e.g. Citizen Kane, to be rated higher on average than recent ones. In the short term, just a few weeks would make a difference on how a movie is perceived. But whether a movie is 50- or 55-year old would be of little impact. In other words, some sort of rescaling of time, logarithmic or other, need considering. If a movie is very good, more people will watch it and rate it. In other words, we should see some correlation between ratings and numbers of ratings. Again, some sort of rescaling of time, logarithmic or other, need considering. Whether this additional information is actually useful will be analysed later in this report. 2.2.1.1 Changes related to the movies: Split the genres tags into separate logical variable, i.e. 1 variable per individual genre. Each individual tags is a -1 or 1 numerical value, with 1 indicating that a movie belongs to that genre. The reasons for using numerical values are: On a more intuitive level, movie are not all-or-nothing of a particular genre: a movie is not funny or not-funny; it could be a little bit funny or extremely funny. We could imagine a dataset where that movie would be a 20% or a 95% Comedy, or -50% anti-funny movie, possibly by extracting information from movies reviews. We could also encode with 0,1 instead of -1,1. Modeling has shown to be more effective with the -1,1 encoding. Key algorithms for recommender system involve dimension reduction which requires all variable to be numerical (no factors). Dimension reduction require variable scaling: for a given movie, all the ratings received by that movie are centered and scaled into a z-score. If a movie only received a single rating, the standard deviation is assumed to be 1 to avoid any missing value. The date a movie came out is extracted from the title of the movie. The date is always a year, which we convert into January, 1st of that year (to avoid any rating being dated before). 2.2.1.2 Changes related to the users: As for the movies, for a given user, ratings given by a particular user are centered and scaled using the mean and standard deviation of all the ratings given by that particular user. 2.2.1.3 Changes related to the dates: Timestamps cannot be readily understood. All dates (including the date a movie came out) are converted to number of properLubridate` date objects. Difference between dates are expressed in days. As we will see, ratings for older movies tend to be higher. Time lapsed until a movie is rated seems of interest (later analysis will show to which extent). The dataset is completed by there time lapses: looking at the date of a particular rating, how many days have passed since: the movie came out; the movie received its first rating; the user gave its first rating. All dates are also in [logarithmic / square root scale]. 2.2.2 Summary of the steps Once the pre-processing is carried out, the dataset variables are: ## [1] &quot;userId&quot; &quot;movieId&quot; &quot;rating&quot; ## [4] &quot;title&quot; &quot;date_rating&quot; &quot;rating_z&quot; ## [7] &quot;movie_nRating&quot; &quot;movie_nRating_log&quot; &quot;movie_mean_rating&quot; ## [10] &quot;movie_sd_rating&quot; &quot;movie_first_rating&quot; &quot;movie_z&quot; ## [13] &quot;movie_year_out&quot; &quot;movie_date_out&quot; &quot;Action&quot; ## [16] &quot;Adventure&quot; &quot;Animation&quot; &quot;Children&quot; ## [19] &quot;Comedy&quot; &quot;Crime&quot; &quot;Documentary&quot; ## [22] &quot;Drama&quot; &quot;Fantasy&quot; &quot;FilmNoir&quot; ## [25] &quot;Horror&quot; &quot;Musical&quot; &quot;Mystery&quot; ## [28] &quot;Romance&quot; &quot;SciFi&quot; &quot;Thriller&quot; ## [31] &quot;War&quot; &quot;Western&quot; &quot;user_nRating&quot; ## [34] &quot;user_nRating_log&quot; &quot;user_mean_rating&quot; &quot;user_sd_rating&quot; ## [37] &quot;user_first_rating&quot; &quot;user_z&quot; &quot;time_since_out&quot; ## [40] &quot;time_since_out_log&quot; &quot;time_movie_first&quot; &quot;time_movie_first_log&quot; ## [43] &quot;time_user_first&quot; &quot;time_user_first_log&quot; See source code.↩ "],
["visualisation.html", "Chapter 3 Visualisation 3.1 Summary analysis of individual variables 3.2 Intuitive statements", " Chapter 3 Visualisation This review is focused on the training set, and excludes the validation data. We are working on the same extract of the full dataset as in the previous section. The purpose of the review is to give a high level sense of what the presented data is and some indicative research avenues for modelling. We first review individual variables. Then we reviews variables by pairs. We have described the Data Preparation section the list of variables that were originally provided, as well as reformatted information. 3.1 Summary analysis of individual variables 3.1.1 Users All users are identified by a single numerical ID to ensure anonymity.5 There are 69750 unique users in the training dataset. Most of them have rated few movies. The following plot shows a log-log plot of number of ratings per user. Recall that the Movie Lens dataset only includes users with 20 or more ratings.6 However, since we are plotting a reduced dataset (20%), we can see users with less than 20 ratings. (#fig:v_rating_per_user)Number of ratings per users (log scale) However, plotting the cumulative sum the number of ratings (as a a number between 0% and 100%) reveals that most of the ratings are provided by a minority of users. (#fig:v_ratings_per_user_cumulative)Cumulative proportion of ratings starting with most active users. We note the movielens data only includes users who have provided at least 20 ratings. 3.1.2 Ratings 3.1.2.1 Ratings are not continuous All ratings are between 0 and 5, say, stars (higher meaning better), using only a whole or half number. A user cannot rate a movie 2.8 or 3.14159. The following code shows that all available ratings apart from 0 have been used. ## # A tibble: 10 x 2 ## rating n ## &lt;dbl&gt; &lt;int&gt; ## 1 0.5 16942 ## 2 1 69045 ## 3 1.5 21447 ## 4 2 142056 ## 5 2.5 66953 ## 6 3 424188 ## 7 3.5 158307 ## 8 4 517704 ## 9 4.5 105809 ## 10 5 277561 We also note that users prefer to use whole numbers instead of half numbers: ## # A tibble: 2 x 2 ## whole_or_half n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 1430554 ## 2 0.5 369458 3.1.2.2 Whole ratings and z-scores Plotting histograms of the ratings are fairly symmetrical with a marked left-skewness (3rd moment of the distribution). (#fig:v_z_score_histograms)Histograms of ratings z-scores (#fig:v_mean_rating_per_genre)Average rating per genre 3.2 Intuitive statements We previously made a number of statements driven by intuition. Let us verify those. 3.2.1 Statement 1 A movie screened for the first time will sometimes be heavily marketed: the decision to watch this movie might be driven by hype rather than a reasoned choice. A plot of ratings during the first 100 days after they come out seems to corroborate the statement: at the far left of the first plot, there is a wide range of ratings (see the width of the smoothing uncertainty band). As time passes by, ratings drops then stabilise. (#fig:v_mean_rating_days_since_out)Ratings for the first 100 days The effect is independent from movie genre (when ignoring all movies that do not have ratings in the early days). (#fig:a_mean_rating_days_since_out_facet)Ratings for the first 100 days by genre 3.2.2 Statement 2 In the medium term after first screening, movie availability could be relevant. Nowadays, the Internet gives access to a huge library of recent and not so recent movies. This was definitely not the case in the years at which ratings started to be collected (mid-nineties). For the purpose of determining whether this statement holds in some way, we need to consider: What happened to the number of ratings over time since a movie came out: more people would see the movie when in movie theaters, whereas later the movies would have been harder to access. Whether these changes in rating numbers vary if a movie is released in the eighties, nineties, and so on. The following plot should be read as follows: choose year on the y-axis, and follow in a straight line from left to right; the colour shows the number of ratings: the darker, the more numerous; the first ratings only in 1988, therefore there is a longer and longer delay before the colours appear when going for later dates to older dates. We can distinguish 4 different zones depending on the first screening date: Very early years before 1992: very few ratings (very pale colour) possibly since fewer people decide to watch older movies. Early years 1993-1996: Strong effect where many ratings are made when the movie is first screen, then very quiet period. Medium years 1996-1998: Very pale in early weeks getting abit darker from 1999 (going down in a diagonal from top-left to bottom right follows a constant year). We can give any intuitive for this, apart from democratisation of the Internet. This is pure conjecture. Recent years 2000 to now: More or less constant colour. Figure 3.1: Number of ratings depending on time lapsed since premier and year of premiering 3.2.3 Statement 3 The decision to watch a movie that came out decades ago is a very deliberate process of choice. There is a survival effect in the sense that time sieved out bad movies. We could expect old movies, e.g. Citizen Kane, to be rated higher on average than recent ones. There is clearly an effect where the average rating goes down. More striking is that recent movies are more likely to receive a bad rating, where the variance of ratings for movies before the early seventies is much lower. This being said, the impact on average movie ratings is fairly small: it goes from just under 4 to mid-3. (#fig:mean_rating_vs_year_out)Average rating depending on the premiering year The statement broadly holds on a genre by genre basis. However, this is clearly not the case for (1) Animation/Children movies (whose quality has dramatically improved and CGI animation clearly caters to a wider audience) and (2) Westerns who have become rarer in recent times and possibly require very strong story/cast to be produced (hence higher average ratings). 3.2.4 Statement 4 In the short term, just a few weeks would make a difference on how a movie is perceived. But whether a movie is 50- or 55-year old would be of little impact. In other words, some sort of rescaling of time, logarithmic or other, need considering. More generally, ratings are more variable in early weeks than later weeks. See Statement 1 plot. 3.2.5 Statement 5 If a movie is very good, many people will watch it and rate it. In other words, we should see some correlation between ratings and numbers of ratings. Again, some sort of rescaling of time, logarithmic or other, need considering. The effect of good movies attracting many spectators is noticeable. It is also very clear that movies with few spectators generate extremely variable results. This effect remains on a genre by genre basis. 3.2.6 Correlations We plotted variable-to-variable correlations. Nothing striking appears: strongly correlated variables are where they chould be (e.g. a variable and its z-score). All interesting correlations are in line with the intuitive statements proposed above. On a reduced set of variables, the plot becomes: Note that in the case of the Netflix challenges, researchers succeeded in de-anonymising part of the dataset by cross-referencing with IMDB information. See (Narayanan and Shmatikov 2006).↩ See the README.html file provided by GroupLens in the zip file.↩ "],
["model.html", "Chapter 4 Model 4.1 Linear regression 4.2 Generalised Linear regression 4.3 LASSO regression 4.4 Conclusion", " Chapter 4 Model From the previous sections, the following variables list_features have been shown to be possibly relevant: ## [1] &quot;rating&quot; &quot;movie_nRating_log&quot; &quot;movie_z&quot; ## [4] &quot;movie_mean_rating&quot; &quot;movie_sd_rating&quot; &quot;user_nRating_log&quot; ## [7] &quot;user_z&quot; &quot;user_mean_rating&quot; &quot;user_sd_rating&quot; ## [10] &quot;movie_year_out&quot; &quot;time_since_out&quot; &quot;time_movie_first_log&quot; ## [13] &quot;time_user_first_log&quot; &quot;Action&quot; &quot;Adventure&quot; ## [16] &quot;Animation&quot; &quot;Children&quot; &quot;Comedy&quot; ## [19] &quot;Crime&quot; &quot;Documentary&quot; &quot;Drama&quot; ## [22] &quot;Fantasy&quot; &quot;FilmNoir&quot; &quot;Horror&quot; ## [25] &quot;Musical&quot; &quot;Mystery&quot; &quot;Romance&quot; ## [28] &quot;SciFi&quot; &quot;Thriller&quot; &quot;War&quot; ## [31] &quot;Western&quot; In this section, we used the reduced and full dataset. However, on all full dataset training attempts, RStudio crashed running out of memory (exceeding 32 GB). # Datasets used for training. # edx_training is either an extract or the full dataset. See source code. x &lt;- edx_training %&gt;% select(one_of(list_features)) %&gt;% as.matrix() # 2.1 GB on full set y &lt;- edx_training %&gt;% select(rating) %&gt;% as.matrix() # The following helper functions: Make a prediction given a fitted model and return the validation dataset with squared error of each prediction. Appends the validation RMSE to a table that will include the 3 models RMSEs. # Squared error of predictions in descending order square_fit &lt;- function(fit_model){ predictions &lt;- fit_model %&gt;% predict(edx_test) return (edx_test %&gt;% cbind(predictions) %&gt;% mutate(square_error = (predictions - rating)^2) %&gt;% arrange(desc(square_error)) ) } RMSEs &lt;- tibble(Model = &quot;Target&quot;, RMSE = 0.8649) add_rmse &lt;- function(name, fit) { rm &lt;- sqrt(sum(fit$square_error) / nrow(fit)) rw &lt;- tibble(Model = name, RMSE = rm) RMSEs %&gt;% rbind(rw) } 4.1 Linear regression The following runs a linear regression on the training data using the predicting variables listed above. set.seed(42, sample.kind = &quot;Rounding&quot;) start_time &lt;- Sys.time() fit_lm &lt;- train(rating ~ ., data = x, method = &quot;lm&quot;) # Make predictions square_lm &lt;- square_fit(fit_lm) RMSEs &lt;- add_rmse(&quot;lm&quot;, square_lm) worst_lm &lt;- square_lm %&gt;% filter(square_error &gt;= 1.5^2) end_time &lt;- Sys.time() print(end_time - start_time) # Results # reduced dataset = 0.8946755 # full dataset = CRASH 4.2 Generalised Linear regression The following runs a generalised linear regression on the training data using the predicting variables listed above. set.seed(42, sample.kind = &quot;Rounding&quot;) start_time &lt;- Sys.time() fit_glm &lt;- train(rating ~ ., data = x, method = &quot;glm&quot;) # Make predictions square_glm &lt;- square_fit(fit_glm) RMSEs &lt;- add_rmse(&quot;glm&quot;, square_glm) worst_glm &lt;- square_glm %&gt;% filter(square_error &gt;= 1.5^2) end_time &lt;- Sys.time() print(end_time - start_time) # Results # reduced dataset = 0.9486 # full dataset = CRASH 4.3 LASSO regression The following runs a regularised linear regression on the training data using the predicting variables listed above. LASSO stands for Least Absolute Shrinkage and Selection Operator. The regularisation operates in two ways: The absolute values of the coeeficients is minimised. Values below a certain threshold are nil-led, effectively removing predictors. # save(fit_lasso, square_lasso, worst_glm, file = &quot;datasets/model_lasso.rda&quot;) # load(&quot;datasets/model_lasso.rda&quot;) set.seed(42, sample.kind = &quot;Rounding&quot;) lambda &lt;- 10^seq(-3, 3, length = 10) fit_lasso &lt;- train( rating ~., data = x, method = &quot;glmnet&quot;, trControl = trainControl(&quot;cv&quot;, number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda) ) # Model coefficients coef(fit_lasso$finalModel, fit_lasso$bestTune$lambda) # Make predictions square_lasso &lt;- square_fit(fit_lasso) RMSEs &lt;- add_rmse(&quot;lasso&quot;, square_lasso) worst_lasso &lt;- square_lasso %&gt;% filter(square_error &gt;= 1.5^2) end_time &lt;- Sys.time() print(end_time - start_time) # Results # reduced dataset = 0.94837 # full dataset = CRASH 4.4 Conclusion Those models, although initially promising, do fail to meet our expectations: They reach an RMSE which is good but not below the threshold of 0.8649. The linear regression model performed best with an RMSE = 0.8946. More importantly, the training and validation on a very small sample of the datasets (20%). The computational resources required to do anything with more data or more sophisticated models has been out of reach (RStudio has crashed numerous times in the process). "],
["stochastic-gradient-descent.html", "Chapter 5 Stochastic Gradient Descent 5.1 Latent factor model 5.2 Formal description 5.3 SGD Code walk", " Chapter 5 Stochastic Gradient Descent The previous models were based on the expectation that our intuitions, confirmed by visual inspection of the dataset, would lead to better performing models. This section shows this is incorrect. We here present a more “brute-force” model: a low-rank matrix factorisation with is approximated by a stochastic gradient descent. This model proves to be very efficient: Before any training, the validation set RMSE is 0.88516 thanks to a non-naive (i.e. not random) initialisation; After very little training, using the initial 3 features (explained below), the RMSE became 0.8304 which improves on the targetted RMSE. A few hours of training brings the RMSE down to 0.7996 with 11 features.7 Visually, the RMSE improvements suggest that additional features may help. 5.1 Latent factor model The approach we follow ia a Latent Factor Model. This section partly draws on part 9 of the Stanford Machine Learning course taught by Andrew Ng (which we previously completed), and a blog post by Sonya Sawtelle.8 In essence, this is a dimension reduction model. But two differences reduce the computational workload: Users and movies are coalesced into groups of similar users and similar movies. This is purely based on the triplets user / movie / rescaled rating. Information about dates, genres is ignored. The model is trained by Stochastic Gradient Descent (SGD). Gradient descent methods are a class of optimisation algorithms that minimise a cost function following downward gradients. SGD is a stochastic version of it where random subsets of the training set are used to converge on very large datasets. 5.2 Formal description 5.2.1 Low-rank factorisation 5.2.1.1 Singular Value Decomposition As noted in the course material (section 34.1.1), singular value decomposition (SVD) is widely used in machine learning. Wikipedia provides a general description which includes a geometric intuition.9 However, this approach is not feasible: the dimensions are too large, the dataset is extremely sparse. 5.2.1.2 Low-rank matrix factorisation (LRMF) At a high level, the purpose of this assignment is to estimate a matrix \\(R\\) of \\(N_{users}\\) rows by \\(N_{movies}\\) columns, where each value contains the rating given by a user to a movie. This is to be estimated from a sample of values from the training set. The intuitive and geometric intuition of LRMF is as follows: Work in a low dimensional space (\\(k\\) dimensions). In that space, give each user \\(u\\) and movie \\(m\\) coordinates in that space (\\(u = (u_1, ..., u_k)\\) and \\(m = (m_1, ..., m_k)\\)). Note that the cross-product of two points in that space will be zero or close to zero if the points are in perpendicular directions. Conversely, points in a close zone in that space will have a cross-product away from zero. In that sense, movies and users can be grouped together in that space: similar movies would be in the same zone of space, different movies would be in perpendicular positions. Because movies ans users both have coordinates in that space, then can all be mixed and grouped: one can measure the similarities between movies, user or between movie and user. The dimensions are commonly called features. In practice, LRMF is represented by two matrices each with \\(k\\) columns: \\(P\\) of \\(N_{users}\\) rows, and \\(Q\\) of \\(N_{users}\\) rows. The \\(k\\) columns give the \\(k\\) coordinates of each user and movie in the feature space. Choosing a user and a movie, the cross-product of the corresponding rows in \\(P\\) and \\(Q\\) gives \\(u \\times m = \\sum_{i = 1}^{k}{u_i m_i}\\) and should produce a rating. The purpose of the algorithm is then to estimate \\(P\\) and \\(Q\\) so that the cross-products match that of the training sets. That is, n matrix notation, \\(R\\) is estimated by \\(P Q^{\\top}\\). 10 It is important to note that the only information used is the rating. Knowledge about the genres of the movies, timestamp of a rating, year a movie premiered is ignored. 5.2.2 Gradient Descent SVD is not useful in our context because (1) the size of the matrices involved is too large, and (2) more importantly requires a fully populated matrix (filling out missing values is a difficult issue). Instead, we will iteratively estimate the \\(P\\) and \\(Q\\) matrices’ coefficients by gradient descent. The cost function used represents prediction error with an additional regularisation cost over those coefficients. 5.2.2.1 Cost function Let’s use the following terms: \\(\\Omega\\) is the set of all \\((user, movie)\\) pairs in the training set; For each \\((u,m)\\) in \\(\\Omega\\), \\(r_{u,m}\\) is the rating in the training set. \\(P\\) is written as \\(p_{i,k}\\), \\(Q\\) is written as \\(q_{j,k}\\) with \\(i \\in [1,...,N_{users}]\\), \\(j \\in [1,...,N_{movies}]\\) and \\(k \\in [1,...,N_{features}]\\). \\(\\lambda\\) is the regularisation parameter. Our regularised cost function is written: \\[ J_{P,Q} = \\sum_{(i, j) \\in \\Omega} {\\left ( r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \\right )^2} + \\frac{\\lambda}{2} \\left ( \\sum_{i,k}{p_{i,k}^2} + \\sum_{j,k}{q_{j,k}^2} \\right ) \\] The gradient descent algorithm seeks to minimise the \\(J_{P,Q}\\) cost function by step-wise update of each model parameter \\(x\\) as follows: \\[ x_{t+1} \\leftarrow x_{t} - \\alpha \\frac{\\partial J_{P,Q}}{\\partial x} \\] The parameters are the matrix coefficients \\(p_{i,k}\\) \\(q_{j,k}\\). \\(\\alpha\\) is the learning parameter that needs to be adjusted. 5.2.2.2 Cost function partial derivatives The partial derivatives of the cost function is: \\[ \\frac{\\partial J_{P,Q}}{\\partial x} = \\frac{\\partial}{\\partial x} \\left ( \\sum_{(i, j) \\in \\Omega} {\\left ( r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \\right )^2} + \\frac{\\lambda}{2} \\left ( \\sum_{i,k}{p_{i,k}^2} + \\sum_{j,k}{q_{j,k}^2} \\right ) \\right ) \\] \\[ \\frac{\\partial J_{P,Q}}{\\partial x} = \\sum_{(i, j) \\in \\Omega} { 2 \\frac{\\partial r_{i,j} - \\sum_{k=1}^{N_{features}} {p_{i,k} q_{j,k}}} {\\partial x} \\left ( r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \\right ) } + \\frac{\\lambda}{2} \\left ( \\sum_{i,k}{2 \\frac{\\partial p_{i,k}}{\\partial x} p_{i,k}} + \\sum_{j,k}{2 \\frac{\\partial p_{i,k}}{\\partial x} q_{j,k}} \\right ) \\] We note that \\(r_{i,j}\\) are constants \\[ \\frac{\\partial J_{P,Q}}{\\partial x} = 2 \\sum_{(i, j) \\in \\Omega} \\sum_{k=1}^{N_{features}} \\left ( { \\frac{\\partial - {p_{i,k} q_{j,k}}}{\\partial x} \\left ( r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \\right ) } \\right ) + \\lambda \\left ( \\sum_{i,k}{\\frac{\\partial p_{i,k}}{\\partial x} p_{i,k}} + \\sum_{j,k}{\\frac{\\partial p_{i,k}}{\\partial x} q_{j,k}} \\right ) \\] If \\(x\\) is a coefficient of \\(P\\) (resp. \\(Q\\)), say \\(p_{a,b}\\) (resp. \\(q_{a,b}\\)), all partial derivatives will be nil unless for \\((i,j) = (a,b)\\). Therefore: \\[ \\frac{\\partial J_{P,Q}}{\\partial p_{a,b}} = -2 \\sum_{(i, j) \\in \\Omega} { q_{j,b} \\left ( r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \\right )} + \\lambda p_{a,b} \\] and, \\[ \\frac{\\partial J_{P,Q}}{\\partial q_{a,b}} = -2 \\sum_{(i, j) \\in \\Omega} { p_{i,b} \\left ( r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \\right ) } + \\lambda q_{a,b} \\] Since \\(\\epsilon{i, j} = r_{i,j} - \\sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}}\\) is the rating prediction error, this becomes: \\[ \\frac{\\partial J_{P,Q}}{\\partial p_{a,b}} = -2 \\sum_{(i, j) \\in \\Omega} { q_{j,b} \\epsilon_{i,j}} + \\lambda p_{a,b} \\] and, \\[ \\frac{\\partial J_{P,Q}}{\\partial q_{a,b}} = -2 \\sum_{(i, j) \\in \\Omega} { p_{i,b} \\epsilon_{i,j} } + \\lambda q_{a,b} \\] 5.2.3 Stochastic Gradient Descent (SGD) The size of the datasets is prohibitive to do those calculations across the entire training set. Instead, we will repeatedly update the model parameters on small random samples of the training set. Chapter 14 of (Shalev-Shwartz and Ben-David 2014) gives an extensive introduction to various SGD algorithms. We implemented a simple version of the algorithm and present the code in more detail. 5.3 SGD Code walk The algorithm is implemented from scratch and relies on nothing but the Tidyverse libraries. library(tidyverse) The quality of the training and predictions is measured by the root mean squared error (RMSE), for which we define a few helper functions (the global variables are defined later): rmse_training &lt;- function(){ prediction_Z &lt;- rowSums(Matrices$P[tri_train$userN,] * Matrices$Q[tri_train$movieN,]) prediction &lt;- prediction_Z * r_sd + r_m sqrt( sum((tri_train$rating - prediction)^2 / nSamples) ) } rmse_validation &lt;- function(){ prediction_Z &lt;- rowSums(Matrices$P[tri_test$userN,] * Matrices$Q[tri_test$movieN,]) prediction &lt;- prediction_Z * r_sd + r_m sqrt( sum((tri_test$rating - prediction)^2) / nTest ) } sum_square &lt;- function(v){ return (sqrt(sum(v^2) / nrow(v))) } The key function updates the model coefficients. Its inputs are: a list that contains the \\(P\\) an \\(Q\\) matrices, the training RMSE of those matrices, and a logical value indicating whether this RMSE is worse than what it was before the update (i.e. did the update diverge). a batch_size that defines the number of samples to be drawn from the training set. A normal gradient descent would use the full training set; by default we only use 10,000 samples out of 10 million (one tenth of a percent). The cost regularisation lambda and gradient descent learning parameter alpha. A number of times to run the descent before recalculating the RMSE and exiting the function (calculating the RMSE is computationally expensive). The training set used is less rich than the original set. As discussed, it only uses the rating (more exactly on the z_score of the rating). Genres, timestamps,… are discarded. # Iterate gradient descent stochastic_grad_descent &lt;- function(model, times = 1, batch_size = 10000, lambda = 0.1, alpha = 0.01, verbose = TRUE) { # Run the descent `times` times. for(i in 1:times) { # Extract a sample of size `batch_size` from the training set. spl &lt;- sample(1:nSamples, size = batch_size, replace = FALSE) spl_training_values &lt;- tri_train[spl,] # Take a subset of `P` and `Q` matching the users and # movies in the training sample. spl_P &lt;- model$P[spl_training_values$userN,] spl_Q &lt;- model$Q[spl_training_values$movieN,] # rowSums returns the cross-product for a given user and movie. # err is the term inside brackets in the partial derivatives # calculation above. err &lt;- spl_training_values$rating_z - rowSums(spl_P * spl_Q) # Partial derivatives wrt p and q delta_P &lt;- -err * spl_Q + lambda * spl_P delta_Q &lt;- -err * spl_P + lambda * spl_Q model$P[spl_training_values$userN,] &lt;- spl_P - alpha * delta_P model$Q[spl_training_values$movieN,] &lt;- spl_Q - alpha * delta_Q } # RMSE against the training set error &lt;- sqrt(sum( (tri_train$rating_z - rowSums(model$P[tri_train$userN,] * model$Q[tri_train$movieN,]))^2) / nSamples ) # Compares to RMSE before update model$WORSE_RMSE &lt;- (model$RMSE &lt; error) model$RMSE &lt;- error # Print some information to keep track of success if (verbose) { cat(&quot; # features=&quot;, ncol(model$P), &quot; J=&quot;, nSamples * error ^2 + lambda/2 * (sum(model$P^2) + sum(model$Q^2)), &quot; Z-scores RMSE=&quot;, model$RMSE, &quot;\\n&quot;) flush.console() } return(model) } Now that the functions are defined, we prepare the data sets. First load the original data if not already available. # Load the datasets which were saved on disk after using the course source code. if(!exists(&quot;edx&quot;)) edx &lt;- readRDS(&quot;datasets/edx.rds&quot;) if(!exists(&quot;validation&quot;)) validation &lt;- readRDS(&quot;datasets/validation.rds&quot;) Calculate the z-score of all ratings. # Creates a movie index from 1 to nMovies r_m &lt;- mean(edx$rating) r_sd &lt;- sd(edx$rating) training_set &lt;- edx %&gt;% select(userId, movieId, rating) %&gt;% mutate(rating_z = (rating - r_m) / r_sd) test_set &lt;- validation %&gt;% select(userId, movieId, rating) %&gt;% mutate(rating_z = (rating - r_m) / r_sd) We do not know if there are any gaps in the userId’s and movieId’s in the datasets. They cannot be used as the row numbers of the \\(P\\) and \\(Q\\) matrices. Therefore we count how many distinct users and movies there are and create an index to link a movieId (resp. userId) to its \\(Q\\) (resp. \\(P\\)) -matrix row number. movieIndex &lt;- training_set %&gt;% distinct(movieId) %&gt;% arrange(movieId) %&gt;% mutate(movieN = row_number()) userIndex &lt;- training_set %&gt;% distinct(userId) %&gt;% arrange(userId) %&gt;% mutate(userN = row_number()) For each movie and user, we calculate its mean rating z-score. movieMean &lt;- training_set %&gt;% group_by(movieId) %&gt;% summarise(m = mean(rating_z)) userMean &lt;- training_set %&gt;% group_by(userId) %&gt;% summarise(m = mean(rating_z)) We can now create the training and validation sets contining the movie index (instead of the movieId), user index and ratings (original and z-score). # Training triplets with z_score tri_train &lt;- training_set %&gt;% left_join(userIndex, by = &quot;userId&quot;) %&gt;% left_join(movieIndex, by = &quot;movieId&quot;) %&gt;% select(-userId, -movieId) tri_test &lt;- test_set %&gt;% select(userId, movieId, rating) %&gt;% left_join(userIndex, by = &quot;userId&quot;) %&gt;% left_join(movieIndex, by = &quot;movieId&quot;) %&gt;% select(-userId, -movieId) %&gt;% mutate(rating_z = (rating - r_m)/r_sd, error = 0) nSamples &lt;- nrow(tri_train) nTest &lt;- nrow(tri_test) nUsers &lt;- tri_train %&gt;% select(userN) %&gt;% n_distinct() nMovies &lt;- tri_train %&gt;% select(movieN) %&gt;% n_distinct() The \\(P\\) and \\(Q\\) matrices are defined with 3 latent factors to start with. # number of initial latent factors nLF &lt;- 3 LF_Model &lt;- list( P = matrix(0, nrow = nUsers, ncol = nLF), Q = matrix(0, nrow = nMovies, ncol = nLF), RMSE = 1000.0, WORSE_RMSE = FALSE) To speed up the training, the matrices are initialised so that the cross product is the sum of the movie average z-rating (\\(m_{movieN}\\)) and user z-rating (\\(u_{userN}\\)). \\[ P \\times Q^{\\top} = \\begin{bmatrix} 1 &amp; u_{1} &amp; 0 \\\\ 1 &amp; u_{2} &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; u_{i} &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; u_{nUser} &amp; 0 \\\\ \\end{bmatrix} \\times \\begin{bmatrix} m_{1} &amp; m_{2} &amp; \\cdots &amp; m_{j} &amp; \\cdots &amp; m_{nMovies} \\\\ 1 &amp; 1 &amp; \\cdots &amp; 1 &amp; \\cdots &amp; 1 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\end{bmatrix} = \\begin{bmatrix} &amp; \\vdots &amp; \\\\ \\cdots &amp; u_{i} + m_{j} &amp; \\cdots \\\\ &amp; \\vdots &amp; \\\\ \\end{bmatrix} \\] # Features matrices are initialised with: # Users: 1st column is 1, 2nd is the mean rating (centered), rest is noise # Movies: 1st column is the mean rating (centered), 2nd is 1, rest is noise # # That way, the matrix multiplication will start by giving reasonable value LF_Model$P[,1] &lt;- matrix(1, nrow = nUsers, ncol = 1) LF_Model$P[,2] &lt;- as.matrix(userIndex %&gt;% left_join(userMean, by =&quot;userId&quot;) %&gt;% select(m)) LF_Model$Q[,1] &lt;- as.matrix(movieIndex %&gt;% left_join(movieMean, by =&quot;movieId&quot;) %&gt;% select(m)) LF_Model$Q[,2] &lt;- matrix(1, nrow = nMovies, ncol = 1) Random noise is added to all model parameters, otherwise the gradient descent has nowhere to start (zeros wipe everything in the matrix multiplications). # Add random noise set.seed(42, sample.kind = &quot;Rounding&quot;) LF_Model$P &lt;- LF_Model$P + matrix(rnorm(nUsers * nLF, mean = 0, sd = 0.01), nrow = nUsers, ncol = nLF) LF_Model$Q &lt;- LF_Model$Q + matrix(rnorm(nMovies * nLF, mean = 0, sd = 0.01), nrow = nMovies, ncol = nLF) We also have a list that keeps track of all the training steps and values. rm(list_results) list_results &lt;- tibble(&quot;alpha&quot; = numeric(), &quot;lambda&quot; = numeric(), &quot;nFeatures&quot; = numeric(), &quot;rmse_training_z_score&quot; = numeric(), &quot;rmse_training&quot; = numeric(), &quot;rmse_validation&quot; = numeric()) The main training loop runs as follows: We start with 3 features. The model is updated in batches of 100 updates. This is done up to 250 times. At each time, if the model starts diverging, the learning parameter (\\(\\alpha\\)) is reduced. Once the 250 times have passed, or if \\(\\alpha\\) has become incredibly small, or if the RMSE doesn’t really improve anymoe (by less than 1 millionth), we add another features and start again. initial_alpha &lt;- 0.1 for(n in 1:100){ # Current number of features number_features &lt;- ncol(LF_Model$P) # lambda = 0.01 for 25 features, i.e. for about 2,000,000 parameters. # We keep lambda proportional to the number of features lambda &lt;- 0.1 * (nUsers + nMovies) * number_features / 2000000 alpha &lt;- initial_alpha cat(&quot;CURRENT FEATURES: &quot;, number_features, &quot;---- Pre-training validation RMSE = &quot;, rmse_validation(), &quot;\\n&quot;) list_results &lt;- list_results %&gt;% add_row(alpha = alpha, lambda = lambda, nFeatures = number_features, rmse_training_z_score = LF_Model$RMSE, rmse_training = rmse_training(), rmse_validation = rmse_validation()) for (i in 1:250){ pre_RMSE &lt;- LF_Model$RMSE LF_Model &lt;- stochastic_grad_descent(model = LF_Model, times = 100, batch_size = 1000 * number_features, alpha = alpha, lambda = lambda) list_results &lt;- list_results %&gt;% add_row(alpha = alpha, lambda = lambda, nFeatures = number_features, rmse_training_z_score = LF_Model$RMSE, rmse_training = rmse_training(), rmse_validation = rmse_validation()) if (LF_Model$WORSE_RMSE) { alpha &lt;- alpha / 2 cat(&quot;Decreasing gradient parameter to: &quot;, alpha, &quot;\\n&quot;) } if (initial_alpha / alpha &gt; 1000 | abs( (LF_Model$RMSE - pre_RMSE) / pre_RMSE) &lt; 1e-6) { break() } } # RMSE against validation set: rmse_validation_post &lt;- rmse_validation() cat(&quot;CURRENT FEATURES: &quot;, number_features, &quot;---- POST-training validation RMSE = &quot;, rmse_validation_post, &quot;\\n&quot;) # if (number_features == 12){ # break() # } # Add k features k_features &lt;- 1 LF_Model$P &lt;- cbind(LF_Model$P, matrix(rnorm(nrow(LF_Model$P) * k_features, mean = 0, sd = sd(LF_Model$P)/100), nrow = nrow(LF_Model$P), ncol = k_features)) LF_Model$Q &lt;- cbind(LF_Model$Q, matrix(rnorm(nrow(LF_Model$Q) * k_features, mean = 0, sd = sd(LF_Model$Q)/100), nrow = nrow(LF_Model$Q), ncol = k_features)) } The following table shows the RMSE on the validation set that is obtained for a given number of features. ## # A tibble: 9 x 2 ## nFeatures best_RMSE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3 0.830 ## 2 4 0.829 ## 3 5 0.818 ## 4 6 0.810 ## 5 7 0.804 ## 6 8 0.804 ## 7 9 0.802 ## 8 10 0.802 ## 9 11 0.800 This plot shows the progress of the RMSE on the validation set. It shows an overall improvement with the number of features, with little worsening spikes each time a feature seeded with random values is added. Figure 5.1: Plot of the RmSE on the validation test We also developed a re-implementation in Julia (availabe on Gihub) that we used to cross-check the R implementation. It gave similar results (in much less time): References "],
["appendix.html", "Chapter 6 Appendix 6.1 Session Info", " Chapter 6 Appendix 6.1 Session Info ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu Focal Fossa (development branch) ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.8.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.8.0 ## ## Random number generation: ## RNG: Mersenne-Twister ## Normal: Inversion ## Sample: Rounding ## ## locale: ## [1] LC_CTYPE=en_AU.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_AU.UTF-8 LC_COLLATE=en_AU.UTF-8 ## [5] LC_MONETARY=en_AU.UTF-8 LC_MESSAGES=en_AU.UTF-8 ## [7] LC_PAPER=en_AU.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] corrplot_0.84 RColorBrewer_1.1-2 kableExtra_1.1.0 ## [4] dslabs_0.7.1 caret_6.0-84 lattice_0.20-38 ## [7] Metrics_0.1.4 gridExtra_2.3 lubridate_1.7.4 ## [10] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 ## [13] purrr_0.3.3 readr_1.3.1 tidyr_1.0.0 ## [16] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.1 jsonlite_1.6 viridisLite_0.3.0 ## [4] splines_3.6.1 foreach_1.4.7 prodlim_2019.10.13 ## [7] modelr_0.1.5 assertthat_0.2.1 highr_0.8 ## [10] stats4_3.6.1 cellranger_1.1.0 yaml_2.2.0 ## [13] ipred_0.9-9 pillar_1.4.2 backports_1.1.5 ## [16] glue_1.3.1 digest_0.6.22 rvest_0.3.5 ## [19] colorspace_1.4-1 recipes_0.1.7 htmltools_0.4.0 ## [22] Matrix_1.2-17 plyr_1.8.4 timeDate_3043.102 ## [25] pkgconfig_2.0.3 broom_0.5.2 haven_2.2.0 ## [28] bookdown_0.14 scales_1.0.0 webshot_0.5.1 ## [31] gower_0.2.1 lava_1.6.6 mgcv_1.8-31 ## [34] generics_0.0.2 withr_2.1.2 nnet_7.3-12 ## [37] lazyeval_0.2.2 cli_1.1.0 survival_3.1-7 ## [40] magrittr_1.5 crayon_1.3.4 readxl_1.3.1 ## [43] evaluate_0.14 fansi_0.4.0 nlme_3.1-142 ## [46] MASS_7.3-51.4 xml2_1.2.2 class_7.3-15 ## [49] tools_3.6.1 data.table_1.12.6 hms_0.5.2 ## [52] lifecycle_0.1.0 munsell_0.5.0 compiler_3.6.1 ## [55] rlang_0.4.1 grid_3.6.1 iterators_1.0.12 ## [58] rstudioapi_0.10 labeling_0.3 rmarkdown_1.16 ## [61] gtable_0.3.0 ModelMetrics_1.2.2 codetools_0.2-16 ## [64] reshape2_1.4.3 R6_2.4.0 knitr_1.25 ## [67] utf8_1.1.4 zeallot_0.1.0 stringi_1.4.3 ## [70] Rcpp_1.0.3 vctrs_0.2.0 rpart_4.1-15 ## [73] tidyselect_0.2.5 xfun_0.10 "],
["references.html", "References", " References "]
]
